## [BERT](https://jalammar.github.io/illustrated-bert/)
![image](https://user-images.githubusercontent.com/67424390/210302799-ca5a509a-c473-43ee-b8a3-cd8fa585166e.png)
#### Step 1: Install Transformer.
#### Step 2: Call the pretrained model.
#### Step 3: Call the tokenizer of that particular pretrained model.
#### Step 4: Convert these encoding into Dataset objects. (Different objects of dataset for tensorflow and pytorch)

### First

* [2_Fine Tuning Pretrained Model On Custom Dataset Using ðŸ¤— Transformer: Custom_Sentiment_Analysis ](https://github.com/krishnaik06/Huggingfacetransformer/blob/main/Custom_Sentiment_Analysis.ipynb)

### Second
* [NLP: Implementing BERT and Transformers from Scratch](https://www.youtube.com/watch?v=EPa98fyxZ-s)
  * [Bert](https://github.com/msaroufim/RLnotes/blob/master/bert.md)
  * [Transformer](https://github.com/msaroufim/RLnotes/blob/master/transformer.md)


### Third
* [ðŸ¤— Transformers Notebooks](https://huggingface.co/docs/transformers/main/en/notebooks)
* [Tutorials by Hugging face for fine tune, processing, etc](https://huggingface.co/docs/transformers/training)

# Refrences 
* https://www.youtube.com/playlist?list=PLZoTAELRMXVOTsz2jZl2Oq3ntWPoKRKwv
* https://neptune.ai/blog/natural-language-processing-with-hugging-face-and-transformers
