Different Types of Word Embeddings 
1. Frequency-based Embedding - Count Vector, TF-IDF Vector
2. Prediction-based Embedding - CBOW (Continuous Bag of words), Skip – Gram model

Word Embedding Algorithms - word2vec, GloVe

## Bag of words and N gram Models
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/0aa66987-d585-4544-898e-2209b3f37077)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/25b9ea54-a669-4036-b780-5a3c748ee0c5)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/621628f9-11f5-42d5-ae6e-de35d69fed13)

### CV and KNN, Random Forest, Multinomial Naive Bayes
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/7298bba3-d210-41b8-96d6-d34f382b3f82)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/70731d64-0160-4a4a-b815-f50067aa0903)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/9feeccfd-cff1-46d8-92dd-7be721f60e5d)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/de317b51-1616-49b1-a30d-8be5c5ce856f)

### Label using KNN
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/30070d2b-7345-43c0-9bf2-bdecc42554bb)

### Balance the imbalanced data with SMOTE
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/712230d3-f270-427c-a0a3-5b41683ba0d5)


![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/04d6f5d8-c7c7-47f7-8a12-ce914b727571)

## TF-IDF
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/69379e1a-414a-4587-ad40-8be667d5b391)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/39057727-4c70-4f9a-b1d3-0f2a57fa90b2)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/27a60ef3-0465-47cf-b0e4-169476027ff5)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/b69965f0-54ba-42d2-a647-22b41ae07bc8)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/878a834c-1efb-4874-9c0a-5438c8fdb38b)

## One hot encoding
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/eabd8652-6a60-4e62-ad19-695c1f17b465)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/dd02fd7a-1e2e-4a33-a1e1-c139fb73d394)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/07704ae8-8a94-4336-a39a-76898ae530d1)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/7372d91d-84c7-4fe0-8742-a591833d5919)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/4752a623-bffd-4ee0-aa7e-6c9558210fc8)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/d5506820-6fae-4485-94fc-574a3383daac)

## Tokenizer

![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/f86de796-bc81-4753-934a-a0b38a05e9e1)

### TokenizerTransformer
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/de6a8973-7938-401f-a14d-299fd8d3d503)

### pad_sequences
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/06415084-0b16-4b69-81b8-68a7368ca8d7)

### Word2Vec -https://www.kaggle.com/code/danielbilitewski/word2vec-and-logistic-regression
Skip-gram: works well with small amount of the training data, represents well even rare words or phrases.
CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words.

![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/ed41b2e9-365f-40bd-82d2-5b23a0ed217b)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/757d5ec6-9336-4cfd-b1f6-8771143eb840)

![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/e20cce20-129d-4942-9e09-0dba2e2dbe9d)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/1eee9de1-f37d-486c-9848-8b70bc70bf20)


1. CBOW (Continuous Bag of words)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/8524315c-7bb1-4cdd-888e-6ae67e3d9132)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/d059fe32-eca1-478f-82f2-bde6ffd5169f)
http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/

2. Skip – Gram model
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/24c10955-649b-4846-8ed5-2cc1f610fc94)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/7f05ae1f-b070-423f-a054-c4c85fb16b20) 
http://www.claudiobellei.com/2018/01/07/backprop-word2vec-python/

![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/31bdb803-72d7-45d2-accf-65253cf2637b)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/203a03f5-a0fd-441a-bcef-7d567b659d7c)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/a2e7a365-6122-401d-9efb-3d7a1306bc12)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/f07ede82-8bf9-4907-bd61-770734d737e5)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/adc56d6d-83ee-45ac-b918-fad8073d2da4)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/061ff9f6-124f-4b69-b4e2-7ab4eb0f3aea)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/6318e081-12b1-4deb-b85b-1a2612f01455)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/a13e9b68-6939-4f29-86e3-36e4f605a281)

### GloVe(Global Vectors for Word Representation) 
https://github.com/vg11072001/NLP-with-Python/blob/master/Toxic%20Comments%20LSTM%20GloVe.ipynb

### fastText
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/b5e9dbe4-c2e8-48b8-8d68-77d336a88506)


## Models
### SimpleRNN
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/c98d2551-2a79-48f5-846c-e7a31ec673ce)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/da902af6-7704-4506-8019-9f5a6a76beaa)


### LSTM
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/695b1bd5-09ee-4c9b-9e74-67f5c6f2df37)
![image](https://github.com/vg11072001/natural-language-processing/assets/67424390/36098e4b-8528-4d9f-bc15-b4f90d247786)
![Uploading image.png…]()




## Named Entity Recognition (NER)


## POS Tagging






Ref - 
https://github.com/codebasics/nlp-tutorials
https://github.com/siddiquiamir/Python-Data-Preprocessing
https://medium.com/@diegoglozano/building-a-pipeline-for-nlp-b569d51db2d1
https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
https://towardsdatascience.com/deep-learning-pipeline-for-natural-language-processing-nlp-c6f4074897bb
https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html
https://www.analyticsvidhya.com/blog/2021/06/practical-guide-to-word-embedding-system/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/
